{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe82b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=8kMaTybvDUw\n",
      "[youtube] 8kMaTybvDUw: Downloading webpage\n",
      "[youtube] 8kMaTybvDUw: Downloading tv client config\n",
      "[youtube] 8kMaTybvDUw: Downloading tv player API JSON\n",
      "[youtube] 8kMaTybvDUw: Downloading ios player API JSON\n",
      "[youtube] 8kMaTybvDUw: Downloading m3u8 information\n",
      "[Downloader] Title: 12-Factor Agents: Patterns of reliable LLM applications — Dex Horthy, HumanLayer\n",
      "[Downloader] Duration: 17:05\n",
      "[Downloader] Metadata saved to: ./metadata\\12-Factor_Agents_Patterns_of_reliable_LLM_applicat.json\n",
      "[Downloader] metadata: {'title': '12-Factor Agents: Patterns of reliable LLM applications\\xa0—\\xa0Dex Horthy, HumanLayer', 'url': 'https://www.youtube.com/watch?v=8kMaTybvDUw', 'uploader': 'AI Engineer', 'duration': 1025, 'upload_date': '20250703'}\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=8kMaTybvDUw\n",
      "[youtube] 8kMaTybvDUw: Downloading webpage\n",
      "[youtube] 8kMaTybvDUw: Downloading tv client config\n",
      "[youtube] 8kMaTybvDUw: Downloading tv player API JSON\n",
      "[youtube] 8kMaTybvDUw: Downloading ios player API JSON\n",
      "[youtube] 8kMaTybvDUw: Downloading m3u8 information\n",
      "[info] 8kMaTybvDUw: Downloading 1 format(s): 251\n",
      "[download] Destination: downloads\\12-Factor Agents： Patterns of reliable LLM applications — Dex Horthy, HumanLayer.webm\n",
      "[download] 100% of   14.05MiB in 00:00:03 at 4.23MiB/s     \n",
      "[ExtractAudio] Destination: downloads\\12-Factor Agents： Patterns of reliable LLM applications — Dex Horthy, HumanLayer.wav\n",
      "Deleting original file downloads\\12-Factor Agents： Patterns of reliable LLM applications — Dex Horthy, HumanLayer.webm (pass -k to keep)\n",
      "Audio downloaded successfully: downloads\\12-Factor Agents： Patterns of reliable LLM applications — Dex Horthy, HumanLayer.wav\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from audio_downloader import download_youtube_audio\n",
    "\n",
    "\n",
    "youtube_url = \"https://www.youtube.com/watch?v=8kMaTybvDUw\"  # Replace with actual URL\n",
    "\n",
    "audio_file = download_youtube_audio(youtube_url)\n",
    "\n",
    "if audio_file:\n",
    "    print(f\"Audio downloaded successfully: {audio_file}\")\n",
    "else:\n",
    "    print(\"Failed to download audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aff3576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found WebM file: 12-Factor Agents： Patterns of reliable LLM applications — Dex Horthy, HumanLayer.wav\n",
      "✅ Found WebM file: 12-Factor Agents： Patterns of reliable LLM applications — Dex Horthy, HumanLayer.wav\n",
      "File size: 187.74 MB\n",
      "Loading Whisper model...\n",
      "✅ Model loaded\n",
      "Transcribing WebM file (this may take a few minutes)...\n",
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102530/102530 [39:27<00:00, 43.30frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transcription completed!\n",
      "✅ Transcription saved to:\n",
      "   JSON: ./transcripts\\12-Factor Agents： Patterns of reliable LLM applications — Dex Horthy, HumanLayer_transcription.json\n",
      "   Text: ./transcripts\\12-Factor Agents： Patterns of reliable LLM applications — Dex Horthy, HumanLayer_transcript.txt\n",
      "\n",
      "Transcription Summary:\n",
      "Language: en\n",
      "Duration: 1017.56 seconds\n",
      "Segments: 351\n",
      "Words: ~3720\n",
      "\n",
      "First few segments:\n",
      "1. [0:00:14] Who here's building agents?...\n",
      "2. [0:00:17] Who here's, leave your hand up if you've built like 10 plus agents....\n",
      "3. [0:00:21] Anyone here built like 100 agents?...\n",
      "\n",
      "🎉 Transcription completed successfully!\n",
      "You can now proceed to the next step: creating embeddings for RAG!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from audio_transcriber import format_and_save_transcription, transcribe_webm_directly\n",
    "# Main execution\n",
    "wav_file = r\"MultiModalRAG\\downloads\\12-Factor Agents： Patterns of reliable LLM applications — Dex Horthy, HumanLayer.wav\"\n",
    "    \n",
    "# Check if file exists (handle the unicode characters in filename)\n",
    "if not os.path.exists(wav_file):\n",
    "    # Try to find WebM files in downloads directory\n",
    "    downloads_dir = \"downloads\"\n",
    "    if os.path.exists(downloads_dir):\n",
    "        wav_files = [f for f in os.listdir(downloads_dir) if f.endswith('.wav')]\n",
    "        if wav_files:\n",
    "            wav_file = os.path.join(downloads_dir, wav_files[0])\n",
    "            print(f\"Found WebM file: {wav_files[0]}\")\n",
    "        else:\n",
    "            print(\"❌ No WebM files found in downloads directory\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(\"❌ Downloads directory not found\")\n",
    "        exit(1)\n",
    "\n",
    "# Transcribe the WebM file\n",
    "result = transcribe_webm_directly(wav_file)\n",
    "\n",
    "if result:\n",
    "    # Format and save\n",
    "    formatted_result = format_and_save_transcription(result, wav_file)\n",
    "    print(\"\\n🎉 Transcription completed successfully!\")\n",
    "    print(\"You can now proceed to the next step: creating embeddings for RAG!\")\n",
    "else:\n",
    "    print(\"❌ Transcription failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f3a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e04b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 24 chunks using LangChain\n",
      "Loading embedding model...\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4809c8e2df54c68bb6b6898b215517d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunking Summary:\n",
      "Total chunks: 24\n",
      "Average words per chunk: 180.4\n",
      "Chunks saved to: ./vectordb_data/video_chunks.json\n",
      "\n",
      "Example chunk:\n",
      "Text: Who here's building agents? Who here's, leave your hand up if you've built like 10 plus agents. Anyone here built like 100 agents? All right, we got a few. Awesome. Love it. So I think a lot of us hav...\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "import json\n",
    "\n",
    "from chunker import process_transcription_for_rag_langchain, save_chunks_for_vectordb\n",
    "\n",
    "\n",
    "metadata_file_path = \"metadata/12-Factor_Agents_Patterns_of_reliable_LLM_applicat.json\"\n",
    "with open(metadata_file_path, 'r', encoding='utf-8') as f:\n",
    "    video_metadata = json.load(f)\n",
    "\n",
    "# Process transcription\n",
    "transcription_file = r\"transcripts\\12-Factor Agents： Patterns of reliable LLM applications — Dex Horthy, HumanLayer_transcription.json\"\n",
    "\n",
    "if os.path.exists(transcription_file):\n",
    "    chunks = process_transcription_for_rag_langchain(transcription_file, video_metadata)\n",
    "    \n",
    "    # Save for vector database\n",
    "    save_chunks_for_vectordb(chunks, \"./vectordb_data/video_chunks.json\")\n",
    "    \n",
    "    # Show example chunk\n",
    "    if chunks:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        example_chunk = chunks[0]\n",
    "        print(f\"Text: {example_chunk['text'][:200]}...\")\n",
    "        print(f\"Embedding dimension: {len(example_chunk['embedding'])}\")\n",
    "else:\n",
    "    print(f\"Transcription file not found: {transcription_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b473551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83605d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7848d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing collection: youtube_videos\n",
      "Added 24 chunks to ChromaDB\n",
      "Using existing collection: youtube_videos\n",
      "Using existing collection: youtube_videos\n",
      "\n",
      "Answer:\n",
      "The main points are about building reliable agents and using frameworks to serve the needs of good builders. The speaker also discusses the 12 factors of building agents and how they can be applied to the practice of building reliable agents. Additionally, the speaker mentions the importance of turning sentences into JSON and the potential harm of using the \"go to\" abstraction in programming languages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Add chunks to ChromaDB\n",
    "from qa import add_chunks_to_chromadb, answer_question, search_videos\n",
    "\n",
    "\n",
    "chunks_file = \"./vectordb_data/video_chunks.json\"\n",
    "import os\n",
    "if os.path.exists(chunks_file):\n",
    "    collection = add_chunks_to_chromadb(chunks_file)\n",
    "    \n",
    "    # Step 2: Test search\n",
    "    query = \"What are the main points?\"\n",
    "    results = search_videos(query)\n",
    "    \n",
    "    # print(f\"\\nSearch results for: '{query}'\")\n",
    "    # for i, doc in enumerate(results['documents'][0]): # type: ignore\n",
    "    #     metadata = results['metadatas'][0][i] # type: ignore\n",
    "    #     print(f\"\\n{i+1}. From: {metadata['video_title']}\")\n",
    "    #     print(f\"Text: {doc[:200]}...\")\n",
    "    \n",
    "    # Step 3: Test Q&A\n",
    "    answer = answer_question(query) \n",
    "    print(f\"\\nAnswer:\\n{answer}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Chunks file not found: {chunks_file}\")\n",
    "    print(\"Please run the chunking step first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "200fbaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing collection: youtube_videos\n",
      "\n",
      "Answer:\n",
      "The top 2 factors mentioned by the speaker are factor one and factor four. \n",
      "\n",
      "Factor one is about the ability of LLMs (Language Model Managers) to turn a sentence into JSON. This factor highlights the power and flexibility of LLMs in handling data and converting it into a usable format. The speaker mentions that this factor is the most magical thing that LLMs can do and it has nothing to do with loops, switch statements, or code. This factor is essential for building reliable and efficient LLM applications. An example of this factor in action would be using an LLM to convert a sentence like \"I am going to the store\" into JSON format, which could look like {\"subject\": \"I\", \"verb\": \"am going\", \"object\": \"store\"}. This conversion can be done quickly and accurately by an LLM, making it a valuable factor for building LLM applications.\n",
      "\n",
      "Factor four is about the use of abstractions in programming languages and the potential harm they can cause. The speaker mentions a paper that discusses the negative effects of using the \"go to\" abstraction in the C programming language. This factor emphasizes the importance of using the right abstractions in programming to avoid creating code that is difficult to maintain and prone to errors. An\n"
     ]
    }
   ],
   "source": [
    "query = \"The speaker mentions 12 factors, can you tell me the top 2 factors? Please provide a detailed answer with examples from the video.\"\n",
    "answer = answer_question(query) \n",
    "print(f\"\\nAnswer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad724949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing collection: youtube_videos\n",
      "\n",
      "Answer: The video is a tutorial on how to make an offline GPT voice assistant in Python. The speaker goes through the steps of setting up the necessary libraries and models, and demonstrates how to use the voice assistant to perform tasks such as taking a screenshot, opening a specific website, and asking and answering questions using a language model. The video also mentions future videos on using machine learning to solve problems.\n",
      "Using existing collection: youtube_videos\n",
      "\n",
      "Answer: Python, GPT, LLM, chat GPT, open AI\n",
      "\n",
      "=== All Processed Videos ===\n",
      "1. Make an Offline GPT Voice Assistant in Python (UUID: e19e81b0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "from audio_downloader import download_youtube_audio\n",
    "from audio_transcriber import transcribe_webm_directly, format_and_save_transcription\n",
    "from chunker import process_transcription_for_rag_langchain, save_chunks_for_vectordb\n",
    "from qa import add_chunks_to_chromadb, answer_question\n",
    "\n",
    "class YouTubeRAGPipeline:\n",
    "    def __init__(self, base_dir=\"./rag_data\"):\n",
    "        self.base_dir = base_dir\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    def process_video(self, youtube_url):\n",
    "        \"\"\"Complete pipeline for a YouTube video\"\"\"\n",
    "        \n",
    "        # Generate unique ID for this video\n",
    "        video_uuid = str(uuid.uuid4())[:8]\n",
    "        print(f\"Processing video with UUID: {video_uuid}\")\n",
    "        \n",
    "        # Create directories for this video\n",
    "        video_dir = os.path.join(self.base_dir, video_uuid)\n",
    "        downloads_dir = os.path.join(video_dir, \"downloads\")\n",
    "        metadata_dir = os.path.join(video_dir, \"metadata\")\n",
    "        transcripts_dir = os.path.join(video_dir, \"transcripts\")\n",
    "        chunks_dir = os.path.join(video_dir, \"chunks\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Download audio\n",
    "            print(\"\\n=== Step 1: Downloading Audio ===\")\n",
    "            audio_file = download_youtube_audio(\n",
    "                youtube_url, \n",
    "                output_path=downloads_dir, \n",
    "                metadata_path=metadata_dir\n",
    "            )\n",
    "            \n",
    "            if not audio_file:\n",
    "                print(\"Failed to download audio\")\n",
    "                return None\n",
    "            \n",
    "            # Step 2: Transcribe audio\n",
    "            print(\"\\n=== Step 2: Transcribing Audio ===\")\n",
    "            result = transcribe_webm_directly(audio_file)\n",
    "            \n",
    "            if not result:\n",
    "                print(\"Failed to transcribe audio\")\n",
    "                return None\n",
    "            \n",
    "            # Save transcription\n",
    "            os.makedirs(transcripts_dir, exist_ok=True)\n",
    "            base_name = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "            transcription_file = os.path.join(transcripts_dir, f\"{base_name}_transcription.json\")\n",
    "            \n",
    "            formatted_result = format_and_save_transcription(result, audio_file)\n",
    "            \n",
    "            # Move transcription to correct location\n",
    "            original_transcript = f\"./transcripts/{base_name}_transcription.json\"\n",
    "            if os.path.exists(original_transcript):\n",
    "                os.rename(original_transcript, transcription_file)\n",
    "            \n",
    "            # Step 3: Load metadata\n",
    "            print(\"\\n=== Step 3: Loading Metadata ===\")\n",
    "            metadata_files = [f for f in os.listdir(metadata_dir) if f.endswith('.json')]\n",
    "            if not metadata_files:\n",
    "                print(\"No metadata file found\")\n",
    "                return None\n",
    "            \n",
    "            metadata_file = os.path.join(metadata_dir, metadata_files[0])\n",
    "            with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "                video_metadata = json.load(f)\n",
    "            \n",
    "            # Add UUID to metadata\n",
    "            video_metadata['uuid'] = video_uuid\n",
    "            \n",
    "            # Step 4: Create chunks\n",
    "            print(\"\\n=== Step 4: Creating Chunks ===\")\n",
    "            os.makedirs(chunks_dir, exist_ok=True)\n",
    "            \n",
    "            chunks = process_transcription_for_rag_langchain(transcription_file, video_metadata)\n",
    "            chunks_file = os.path.join(chunks_dir, \"video_chunks.json\")\n",
    "            save_chunks_for_vectordb(chunks, chunks_file)\n",
    "            \n",
    "            # Step 5: Add to ChromaDB\n",
    "            print(\"\\n=== Step 5: Adding to Vector Database ===\")\n",
    "            add_chunks_to_chromadb(chunks_file)\n",
    "            \n",
    "            print(f\"\\n✅ Successfully processed video!\")\n",
    "            print(f\"Video UUID: {video_uuid}\")\n",
    "            print(f\"Title: {video_metadata['title']}\")\n",
    "            print(f\"Data saved in: {video_dir}\")\n",
    "            \n",
    "            return {\n",
    "                'uuid': video_uuid,\n",
    "                'metadata': video_metadata,\n",
    "                'audio_file': audio_file,\n",
    "                'transcription_file': transcription_file,\n",
    "                'chunks_file': chunks_file\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in pipeline: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def ask_question(self, question):\n",
    "        \"\"\"Ask a question across all processed videos\"\"\"\n",
    "        return answer_question(question)\n",
    "    \n",
    "    def list_videos(self):\n",
    "        \"\"\"List all processed videos\"\"\"\n",
    "        videos = []\n",
    "        if not os.path.exists(self.base_dir):\n",
    "            return videos\n",
    "        \n",
    "        for video_uuid in os.listdir(self.base_dir):\n",
    "            video_dir = os.path.join(self.base_dir, video_uuid)\n",
    "            metadata_dir = os.path.join(video_dir, \"metadata\")\n",
    "            \n",
    "            if os.path.exists(metadata_dir):\n",
    "                metadata_files = [f for f in os.listdir(metadata_dir) if f.endswith('.json')]\n",
    "                if metadata_files:\n",
    "                    metadata_file = os.path.join(metadata_dir, metadata_files[0])\n",
    "                    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "                        metadata = json.load(f)\n",
    "                    videos.append({\n",
    "                        'uuid': video_uuid,\n",
    "                        'title': metadata.get('title', 'Unknown'),\n",
    "                        'uploader': metadata.get('uploader', 'Unknown'),\n",
    "                        'url': metadata.get('url', '')\n",
    "                    })\n",
    "        \n",
    "        return videos\n",
    "\n",
    "    # Initialize pipeline\n",
    "pipeline = YouTubeRAGPipeline()\n",
    "\n",
    "# Process a video\n",
    "youtube_url = \"https://www.youtube.com/watch?v=w5unVTO7mLQ\"\n",
    "\n",
    "# result = pipeline.process_video(youtube_url)\n",
    "\n",
    "if result:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Interactive Q&A\n",
    "    while True:\n",
    "        question = input(\"\\nAsk a question (or 'quit' to exit): \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        answer = pipeline.ask_question(question)\n",
    "        print(f\"\\nAnswer: {answer}\")\n",
    "\n",
    "# Show all videos\n",
    "print(\"\\n=== All Processed Videos ===\")\n",
    "videos = pipeline.list_videos()\n",
    "for i, video in enumerate(videos, 1):\n",
    "    print(f\"{i}. {video['title']} (UUID: {video['uuid']})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7162fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd84be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1640c847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Video already processed!\n",
      "UUID: e19e81b0\n",
      "Title: Make an Offline GPT Voice Assistant in Python\n",
      "Loading existing data into ChromaDB...\n",
      "Using existing collection: youtube_videos\n",
      "Added 27 chunks to ChromaDB\n",
      "✅ Data loaded successfully!\n",
      "\n",
      "==================================================\n",
      "Ready for Q&A!\n",
      "==================================================\n",
      "Using existing collection: youtube_videos\n",
      "\n",
      "Question: what is this video about?\n",
      "Answer: The video is about creating an offline GPT voice assistant in Python.\n",
      "Using existing collection: youtube_videos\n",
      "\n",
      "Question: how many people are speaking?\n",
      "Answer: One person is speaking in the video transcript.\n",
      "Using existing collection: youtube_videos\n",
      "\n",
      "Question: for speech recogniton what is getting used?\n",
      "Answer: For speech recognition, the speech recognition library is being used in Python.\n",
      "Using existing collection: youtube_videos\n",
      "\n",
      "Question: for speech recogniton what is geting used?\n",
      "Answer: For speech recognition, the speech recognition package in Python is being used.\n"
     ]
    }
   ],
   "source": [
    "pipeline = YouTubeRAGPipeline()\n",
    "import os\n",
    "# Process a video\n",
    "youtube_url = \"https://www.youtube.com/watch?v=w5unVTO7mLQ\"\n",
    "\n",
    "# Check if video already processed\n",
    "videos = pipeline.list_videos()\n",
    "existing_video = None\n",
    "for video in videos:\n",
    "    if video['url'] == youtube_url:\n",
    "        existing_video = video\n",
    "        break\n",
    "\n",
    "if existing_video:\n",
    "    print(f\"✅ Video already processed!\")\n",
    "    print(f\"UUID: {existing_video['uuid']}\")\n",
    "    print(f\"Title: {existing_video['title']}\")\n",
    "    print(\"Loading existing data into ChromaDB...\")\n",
    "    \n",
    "    # Load existing chunks into ChromaDB\n",
    "    chunks_file = f\"./rag_data/{existing_video['uuid']}/chunks/video_chunks.json\"\n",
    "    if os.path.exists(chunks_file):\n",
    "        from qa import add_chunks_to_chromadb\n",
    "        add_chunks_to_chromadb(chunks_file)\n",
    "        print(\"✅ Data loaded successfully!\")\n",
    "    else:\n",
    "        print(\"❌ Chunks file not found\")\n",
    "        \n",
    "else:\n",
    "    print(\"🔄 Video not found. Processing new video...\")\n",
    "    result = pipeline.process_video(youtube_url)\n",
    "    if not result:\n",
    "        print(\"❌ Failed to process video\")\n",
    "        exit()\n",
    "    print(f\"✅ Video processed successfully!\")\n",
    "    print(f\"UUID: {result['uuid']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Ready for Q&A!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nAsk a question (or 'quit' to exit): \")\n",
    "    if question.lower() == 'quit':\n",
    "        break\n",
    "        \n",
    "    answer = pipeline.ask_question(question)\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2c92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb677f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Fast Processing Mode\n",
      "==================================================\n",
      "\n",
      "📝 Step 1: Getting transcript...\n",
      "🔍 Fetching transcript for video ID: w5unVTO7mLQ\n",
      "✅ Found manual captions\n",
      "✅ Transcript extracted successfully!\n",
      "Language: English\n",
      "Generated: False\n",
      "Duration: 1468.5 seconds\n",
      "Segments: 899\n",
      "\n",
      "📊 Step 2: Getting metadata...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yt_dlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 198\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m    196\u001b[0m youtube_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.youtube.com/watch?v=w5unVTO7mLQ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 198\u001b[0m transcript_data, metadata \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_youtube_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43myoutube_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transcript_data \u001b[38;5;129;01mand\u001b[39;00m metadata:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📋 Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 181\u001b[0m, in \u001b[0;36mprocess_youtube_fast\u001b[1;34m(youtube_url)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Step 2: Get metadata (fast)\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 Step 2: Getting metadata...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_video_metadata_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43myoutube_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m metadata:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Could not get video metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 135\u001b[0m, in \u001b[0;36mget_video_metadata_fast\u001b[1;34m(youtube_url)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_video_metadata_fast\u001b[39m(youtube_url):\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Get basic video metadata without downloading\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    Uses yt-dlp extract_info with download=False\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myt_dlp\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     ydl_opts \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquiet\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_warnings\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    140\u001b[0m     }\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'yt_dlp'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from youtube_transcript_api._api import YouTubeTranscriptApi\n",
    "import json\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "def extract_video_id(youtube_url):\n",
    "    \"\"\"Extract video ID from YouTube URL\"\"\"\n",
    "    patterns = [\n",
    "        r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*',\n",
    "        r'(?:embed\\/)([0-9A-Za-z_-]{11})',\n",
    "        r'(?:youtu\\.be\\/)([0-9A-Za-z_-]{11})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, youtube_url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "def get_youtube_transcript_fast(youtube_url):\n",
    "    \"\"\"\n",
    "    Fast transcript extraction using YouTube's existing captions\n",
    "    Falls back to None if no captions available\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract video ID\n",
    "    video_id = extract_video_id(youtube_url)\n",
    "    if not video_id:\n",
    "        print(\"❌ Invalid YouTube URL\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"🔍 Fetching transcript for video ID: {video_id}\")\n",
    "        \n",
    "        # Try to get transcript (auto-generated or manual)\n",
    "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "        \n",
    "        # Prefer manual captions over auto-generated\n",
    "        transcript = None\n",
    "        try:\n",
    "            # Try manual captions first\n",
    "            transcript = transcript_list.find_manually_created_transcript(['en'])\n",
    "            print(\"✅ Found manual captions\")\n",
    "        except:\n",
    "            try:\n",
    "                # Fall back to auto-generated\n",
    "                transcript = transcript_list.find_generated_transcript(['en'])\n",
    "                print(\"✅ Found auto-generated captions\")\n",
    "            except:\n",
    "                # Try any available language\n",
    "                available_transcripts = list(transcript_list)\n",
    "                if available_transcripts:\n",
    "                    transcript = available_transcripts[0]\n",
    "                    print(f\"✅ Found transcript in {transcript.language}\")\n",
    "        \n",
    "        if not transcript:\n",
    "            print(\"❌ No transcripts available\")\n",
    "            return None\n",
    "        \n",
    "        # Fetch the transcript\n",
    "        transcript_data = transcript.fetch()\n",
    "        \n",
    "        # Format the data\n",
    "        formatted_transcript = {\n",
    "            'language': transcript.language,\n",
    "            'is_generated': transcript.is_generated,\n",
    "            'segments': [],\n",
    "            'full_text': '',\n",
    "            'total_duration': 0\n",
    "        }\n",
    "        \n",
    "        full_text_parts = []\n",
    "        \n",
    "        for item in transcript_data:\n",
    "            segment = {\n",
    "                'start_time': str(timedelta(seconds=int(item.start))),\n",
    "                'end_time': str(timedelta(seconds=int(item.start + item.duration))),\n",
    "                'start_seconds': item.start,\n",
    "                'end_seconds': item.start + item.duration,\n",
    "                'text': item.text.strip(),\n",
    "                'duration': item.duration\n",
    "            }\n",
    "            \n",
    "            formatted_transcript['segments'].append(segment)\n",
    "            full_text_parts.append(item.text)\n",
    "        \n",
    "        formatted_transcript['full_text'] = ' '.join(full_text_parts)\n",
    "        if formatted_transcript['segments']:\n",
    "            formatted_transcript['total_duration'] = formatted_transcript['segments'][-1]['end_seconds']\n",
    "        \n",
    "        print(f\"✅ Transcript extracted successfully!\")\n",
    "        print(f\"Language: {formatted_transcript['language']}\")\n",
    "        print(f\"Generated: {formatted_transcript['is_generated']}\")\n",
    "        print(f\"Duration: {formatted_transcript['total_duration']:.1f} seconds\")\n",
    "        print(f\"Segments: {len(formatted_transcript['segments'])}\")\n",
    "        \n",
    "        return formatted_transcript\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching transcript: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def save_fast_transcript(transcript_data, youtube_url, output_dir=\"./transcripts\"):\n",
    "    \"\"\"Save the fast transcript to JSON file\"\"\"\n",
    "    \n",
    "    if not transcript_data:\n",
    "        return None\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create filename from video ID\n",
    "    video_id = extract_video_id(youtube_url)\n",
    "    filename = f\"fast_transcript_{video_id}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Add source info\n",
    "    transcript_data['source'] = 'youtube_captions'\n",
    "    transcript_data['video_url'] = youtube_url\n",
    "    transcript_data['video_id'] = video_id\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(transcript_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"💾 Transcript saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def get_video_metadata_fast(youtube_url):\n",
    "    \"\"\"\n",
    "    Get basic video metadata without downloading\n",
    "    Uses yt-dlp extract_info with download=False\n",
    "    \"\"\"\n",
    "    import yt_dlp  # type: ignore\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'no_warnings': True,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(youtube_url, download=False)\n",
    "            \n",
    "            metadata = {\n",
    "                'title': info.get('title', 'Unknown'),\n",
    "                'url': youtube_url,\n",
    "                'uploader': info.get('uploader', 'Unknown'),\n",
    "                'duration': info.get('duration', 0),\n",
    "                'upload_date': info.get('upload_date', ''),\n",
    "                'view_count': info.get('view_count', 0),\n",
    "                'description': info.get('description', '')[:500] + '...' if info.get('description') else ''\n",
    "            }\n",
    "            \n",
    "            return metadata\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error getting metadata: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Combined fast processing function\n",
    "def process_youtube_fast(youtube_url):\n",
    "    \"\"\"\n",
    "    Fast processing: Get transcript + metadata without downloading audio\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 Fast Processing Mode\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Step 1: Get transcript (fast)\n",
    "    print(\"\\n📝 Step 1: Getting transcript...\")\n",
    "    transcript_data = get_youtube_transcript_fast(youtube_url)\n",
    "    \n",
    "    if not transcript_data:\n",
    "        print(\"❌ No captions available. Would need to use Whisper (slow mode).\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 2: Get metadata (fast)\n",
    "    print(\"\\n📊 Step 2: Getting metadata...\")\n",
    "    metadata = get_video_metadata_fast(youtube_url)\n",
    "    \n",
    "    if not metadata:\n",
    "        print(\"❌ Could not get video metadata\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 3: Save transcript\n",
    "    print(\"\\n💾 Step 3: Saving transcript...\")\n",
    "    transcript_file = save_fast_transcript(transcript_data, youtube_url)\n",
    "    \n",
    "    print(\"\\n✅ Fast processing complete!\")\n",
    "    print(f\"Time saved: ~5-10 minutes vs Whisper approach\")\n",
    "    \n",
    "    return transcript_data, metadata\n",
    "# Example usage\n",
    "youtube_url = \"https://www.youtube.com/watch?v=w5unVTO7mLQ\"\n",
    "\n",
    "transcript_data, metadata = process_youtube_fast(youtube_url)\n",
    "\n",
    "if transcript_data and metadata:\n",
    "    print(f\"\\n📋 Summary:\")\n",
    "    print(f\"Title: {metadata['title']}\")\n",
    "    print(f\"Duration: {transcript_data['total_duration']:.1f} seconds\")\n",
    "    print(f\"Language: {transcript_data['language']}\")\n",
    "    print(f\"Word count: ~{len(transcript_data['full_text'].split())}\")\n",
    "    print(f\"First 100 chars: {transcript_data['full_text'][:100]}...\")\n",
    "else:\n",
    "    print(\"❌ Fast processing failed - would need Whisper fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1e483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4847b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING TRANSCRIPT: rag_data\\eb4371eb\\transcripts\\transcript.json ===\n",
      "📋 Top-level keys:\n",
      "  - language: <class 'str'>\n",
      "  - full_text: <class 'str'>\n",
      "  - segments: <class 'list'>\n",
      "  - total_duration: <class 'float'>\n",
      "  - source_type: <class 'str'>\n",
      "\n",
      "📝 Full text preview:\n",
      "  Length: 27364\n",
      "  Preview: In this video, we're going to make an offline virtual assistant that uses a local LLM, just like cha...\n",
      "\n",
      "🎬 Segments info:\n",
      "  Total segments: 899\n",
      "  First segment keys: ['start_time', 'end_time', 'start_seconds', 'end_seconds', 'text', 'duration']\n",
      "  First segment: {'start_time': '0:00:00', 'end_time': '0:00:01', 'start_seconds': 0.016, 'end_seconds': 1.733, 'text': \"In this video, we're going to make an\", 'duration': 1.717}\n",
      "\n",
      "  Sample of first 3 segments:\n",
      "    1. {'start_time': '0:00:00', 'end_time': '0:00:01', 'start_seconds': 0.016, 'end_seconds': 1.733, 'text': \"In this video, we're going to make an\", 'duration': 1.717}\n",
      "    2. {'start_time': '0:00:01', 'end_time': '0:00:03', 'start_seconds': 1.733, 'end_seconds': 3.366, 'text': 'offline virtual assistant', 'duration': 1.633}\n",
      "    3. {'start_time': '0:00:03', 'end_time': '0:00:06', 'start_seconds': 3.366, 'end_seconds': 6.633, 'text': 'that uses a local LLM, just', 'duration': 3.267}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def debug_transcript_structure(transcription_file: str):\n",
    "    \"\"\"Debug function to see what's actually in the transcript file\"\"\"\n",
    "    \n",
    "    print(f\"=== DEBUGGING TRANSCRIPT: {transcription_file} ===\")\n",
    "    \n",
    "    with open(transcription_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"📋 Top-level keys:\")\n",
    "    for key in data.keys():\n",
    "        print(f\"  - {key}: {type(data[key])}\")\n",
    "    \n",
    "    print(f\"\\n📝 Full text preview:\")\n",
    "    full_text = data.get('full_text', 'NOT FOUND')\n",
    "    print(f\"  Length: {len(full_text) if full_text != 'NOT FOUND' else 'N/A'}\")\n",
    "    print(f\"  Preview: {full_text[:100] if full_text != 'NOT FOUND' else 'N/A'}...\")\n",
    "    \n",
    "    print(f\"\\n🎬 Segments info:\")\n",
    "    segments = data.get('segments', [])\n",
    "    print(f\"  Total segments: {len(segments)}\")\n",
    "    \n",
    "    if segments:\n",
    "        print(f\"  First segment keys: {list(segments[0].keys())}\")\n",
    "        print(f\"  First segment: {segments[0]}\")\n",
    "        \n",
    "        print(f\"\\n  Sample of first 3 segments:\")\n",
    "        for i, segment in enumerate(segments[:3]):\n",
    "            print(f\"    {i+1}. {segment}\")\n",
    "    else:\n",
    "        print(\"  ❌ NO SEGMENTS FOUND!\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Test with your transcript file\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual transcript file path\n",
    "    transcript_file = r\"rag_data\\eb4371eb\\transcripts\\transcript.json\"\n",
    "    \n",
    "    try:\n",
    "        debug_transcript_structure(transcript_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ File not found: {transcript_file}\")\n",
    "        print(\"Please update the path to your actual transcript file\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f76a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing collection: video_eb4371eb\n",
      "\n",
      "🔍 ChromaDB Debug for collection: video_eb4371eb\n",
      "📊 Total items in collection: 35\n",
      "📋 Metadata keys in first item: ['word_count', 'uploader', 'video_title', 'chunk_id', 'video_url']\n",
      "🕐 First item timestamps: None - None\n",
      "  Item 1: None - None | Make an Offline GPT Voice Assistant in Python\n",
      "  Item 2: None - None | Make an Offline GPT Voice Assistant in Python\n",
      "  Item 3: None - None | Make an Offline GPT Voice Assistant in Python\n",
      "Using existing collection: video_eb4371eb\n",
      "\n",
      "🔍 Debug: Found 3 results\n",
      "📍 First result metadata keys: ['video_url', 'word_count', 'uploader', 'video_title', 'chunk_id']\n",
      "🕐 First result timestamps: None - None\n",
      "\n",
      "Answer: The version of whisper used in the video transcript is not explicitly mentioned. The video focuses on utilizing the whisper API from OpenAI for speech recognition [Source 3]. The whisper API is specifically mentioned as being from OpenAI, the company responsible for chat GPT [Source 3]. The video demonstrates how to transcribe audio using the whisper model and the API provided by OpenAI [Source 2].\n",
      "\n",
      "==================================================\n",
      "Sources:\n",
      "Source 1: 'Make an Offline GPT Voice Assistant in Python' at Unknown-Unknown\n",
      "Source 2: 'Make an Offline GPT Voice Assistant in Python' at Unknown-Unknown\n",
      "Source 3: 'Make an Offline GPT Voice Assistant in Python' at Unknown-Unknown\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def setup_chromadb(collection_name=\"youtube_videos\"):\n",
    "    \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    \n",
    "    # Get or create collection\n",
    "    try:\n",
    "        collection = client.get_collection(collection_name)\n",
    "        print(f\"Using existing collection: {collection_name}\")\n",
    "    except:\n",
    "        collection = client.create_collection(collection_name)\n",
    "        print(f\"Created new collection: {collection_name}\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "def add_chunks_to_chromadb(chunks_file: str, collection_name=\"youtube_videos\"):\n",
    "    \"\"\"Load chunks and add them to ChromaDB with citation info\"\"\"\n",
    "    \n",
    "    # Load chunks\n",
    "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "    \n",
    "    # Setup ChromaDB\n",
    "    collection = setup_chromadb(collection_name)\n",
    "    \n",
    "    # Prepare data for ChromaDB\n",
    "    ids = []\n",
    "    embeddings = []\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        ids.append(chunk['unique_id'])\n",
    "        embeddings.append(chunk['embedding'])\n",
    "        documents.append(chunk['text'])\n",
    "        \n",
    "        # Metadata with citation tracking info\n",
    "        metadata = {\n",
    "            'chunk_id': chunk['chunk_id'],\n",
    "            'video_title': chunk['video_title'],\n",
    "            'video_url': chunk['video_url'],\n",
    "            'uploader': chunk['uploader'],\n",
    "            'word_count': chunk['word_count'],\n",
    "            \n",
    "            # Citation tracking fields - IMPORTANT: Convert to strings for ChromaDB\n",
    "            'start_time': chunk.get('start_time', '0:00:00'),\n",
    "            'end_time': chunk.get('end_time', '0:00:00'),\n",
    "            'start_seconds': float(chunk.get('start_seconds', 0)),\n",
    "            'end_seconds': float(chunk.get('end_seconds', 0)),\n",
    "            'duration': float(chunk.get('duration', 0))\n",
    "        }\n",
    "        metadatas.append(metadata)\n",
    "    \n",
    "    # Add to ChromaDB\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=embeddings,\n",
    "        documents=documents,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to ChromaDB with citation tracking\")\n",
    "    print(f\"✅ First chunk citation example: {metadatas[0]['start_time']} - {metadatas[0]['end_time']}\")\n",
    "    return collection\n",
    "\n",
    "def search_videos(query: str, collection_name=\"youtube_videos\", n_results=3):\n",
    "    \"\"\"Search for relevant video chunks\"\"\"\n",
    "    \n",
    "    # Setup ChromaDB\n",
    "    collection = setup_chromadb(collection_name)\n",
    "    \n",
    "    # Search\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def answer_question_with_citations(query: str, collection_name=\"youtube_videos\"):\n",
    "    \"\"\"Enhanced Q&A with proper citation tracking\"\"\"\n",
    "    \n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    \n",
    "    # Search for relevant chunks\n",
    "    results = search_videos(query, collection_name, n_results=3)\n",
    "    \n",
    "    if not results['documents'][0]:\n",
    "        return \"No relevant information found.\"\n",
    "    \n",
    "    # Debug: Check what we got back\n",
    "    print(f\"\\n🔍 Debug: Found {len(results['documents'][0])} results\")\n",
    "    if results['metadatas'][0]:\n",
    "        first_meta = results['metadatas'][0][0]\n",
    "        print(f\"📍 First result metadata keys: {list(first_meta.keys())}\")\n",
    "        print(f\"🕐 First result timestamps: {first_meta.get('start_time')} - {first_meta.get('end_time')}\")\n",
    "    \n",
    "    # Prepare context with citation info\n",
    "    context_chunks = []\n",
    "    citations = []\n",
    "    \n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        metadata = results['metadatas'][0][i]\n",
    "        \n",
    "        # Extract citation info safely\n",
    "        video_title = metadata.get('video_title', 'Unknown Video')\n",
    "        start_time = metadata.get('start_time', 'Unknown')\n",
    "        end_time = metadata.get('end_time', 'Unknown') \n",
    "        video_url = metadata.get('video_url', '')\n",
    "        start_seconds = metadata.get('start_seconds', 0)\n",
    "        \n",
    "        # Create citation with clickable timestamp link\n",
    "        if video_url and start_seconds and start_time != 'Unknown':\n",
    "            timestamped_url = f\"{video_url}&t={int(start_seconds)}s\"\n",
    "            citation = f\"Source {i+1}: '{video_title}' at {start_time}-{end_time} ({timestamped_url})\"\n",
    "        else:\n",
    "            citation = f\"Source {i+1}: '{video_title}' at {start_time}-{end_time}\"\n",
    "        \n",
    "        citations.append(citation)\n",
    "        context_chunks.append(f\"[Source {i+1}] {doc}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    # Generate answer with citations\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"\n",
    "Based on the following video transcript context, answer the question and include source references.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer with citations (use [Source X] format):\"\"\"\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(prompt.format(context=context, question=query))\n",
    "    answer = response.content.strip()\n",
    "    \n",
    "    # Append full citations\n",
    "    full_response = f\"{answer}\\n\\n\" + \"=\"*50 + \"\\nSources:\\n\" + \"\\n\".join(citations)\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "def answer_question(query: str, collection_name=\"youtube_videos\"):\n",
    "    \"\"\"Wrapper function to maintain compatibility\"\"\"\n",
    "    return answer_question_with_citations(query, collection_name)\n",
    "\n",
    "# Debug function to check what's in ChromaDB\n",
    "def debug_chromadb_metadata(collection_name=\"youtube_videos\"):\n",
    "    \"\"\"Debug function to see what metadata is actually stored\"\"\"\n",
    "    \n",
    "    collection = setup_chromadb(collection_name)\n",
    "    \n",
    "    # Get a few items to check metadata\n",
    "    results = collection.get(limit=3)\n",
    "    \n",
    "    print(f\"\\n🔍 ChromaDB Debug for collection: {collection_name}\")\n",
    "    print(f\"📊 Total items in collection: {collection.count()}\")\n",
    "    \n",
    "    if results['metadatas']:\n",
    "        print(f\"📋 Metadata keys in first item: {list(results['metadatas'][0].keys())}\")\n",
    "        print(f\"🕐 First item timestamps: {results['metadatas'][0].get('start_time')} - {results['metadatas'][0].get('end_time')}\")\n",
    "        \n",
    "        for i, meta in enumerate(results['metadatas'][:3]):\n",
    "            print(f\"  Item {i+1}: {meta.get('start_time')} - {meta.get('end_time')} | {meta.get('video_title', 'No title')}\")\n",
    "    else:\n",
    "        print(\"❌ No metadata found!\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Debug what's in ChromaDB\n",
    "    debug_chromadb_metadata(\"video_eb4371eb\")  # Replace with your actual collection name\n",
    "    \n",
    "    # Test a query\n",
    "    query = \"What version of whisper is used?\"\n",
    "    answer = answer_question_with_citations(query, \"video_eb4371eb\")\n",
    "    print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4160bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb247e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d382efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3697bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53f830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing timestamp mapping function\n",
      "==================================================\n",
      "📝 Sample chunk: In this video, we're going to make an offline virtual assistant that uses a local LLM\n",
      "🎬 Number of segments: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravis\\OneDrive\\Desktop\\PROJECTS\\LLMProjectsFromLinkedin\\VideoQARAG\\videorag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Function result: {'start_time': '0:00:00', 'end_time': '0:00:06', 'start_seconds': 0.016, 'end_seconds': 6.633}\n",
      "🕐 Start time: 0:00:00\n",
      "🕐 End time: 0:00:06\n",
      "⏱️  Start seconds: 0.016\n",
      "⏱️  End seconds: 6.633\n",
      "✅ SUCCESS: Timestamps mapped correctly!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def test_timestamp_mapping():\n",
    "    \"\"\"Test the timestamp mapping function directly\"\"\"\n",
    "    \n",
    "    # Sample transcript data (like your format)\n",
    "    sample_segments = [\n",
    "        {\n",
    "            'start_time': '0:00:00',\n",
    "            'end_time': '0:00:01', \n",
    "            'start_seconds': 0.016,\n",
    "            'end_seconds': 1.733,\n",
    "            'text': \"In this video, we're going to make an\",\n",
    "            'duration': 1.717\n",
    "        },\n",
    "        {\n",
    "            'start_time': '0:00:01',\n",
    "            'end_time': '0:00:03',\n",
    "            'start_seconds': 1.733, \n",
    "            'end_seconds': 3.366,\n",
    "            'text': 'offline virtual assistant',\n",
    "            'duration': 1.633\n",
    "        },\n",
    "        {\n",
    "            'start_time': '0:00:03',\n",
    "            'end_time': '0:00:06',\n",
    "            'start_seconds': 3.366,\n",
    "            'end_seconds': 6.633, \n",
    "            'text': 'that uses a local LLM, just',\n",
    "            'duration': 3.267\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Sample chunk text\n",
    "    sample_chunk = \"In this video, we're going to make an offline virtual assistant that uses a local LLM\"\n",
    "    \n",
    "    print(\"🧪 Testing timestamp mapping function\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"📝 Sample chunk: {sample_chunk}\")\n",
    "    print(f\"🎬 Number of segments: {len(sample_segments)}\")\n",
    "    \n",
    "    # Import and test the function\n",
    "    try:\n",
    "        from chunker import find_chunk_timestamps\n",
    "        \n",
    "        result = find_chunk_timestamps(sample_chunk, sample_segments)\n",
    "        \n",
    "        print(f\"\\n✅ Function result: {result}\")\n",
    "        print(f\"🕐 Start time: {result['start_time']}\")\n",
    "        print(f\"🕐 End time: {result['end_time']}\")\n",
    "        print(f\"⏱️  Start seconds: {result['start_seconds']}\")\n",
    "        print(f\"⏱️  End seconds: {result['end_seconds']}\")\n",
    "        \n",
    "        if result['start_time'] == '0:00:00' and result['end_time'] != '0:00:00':\n",
    "            print(\"✅ SUCCESS: Timestamps mapped correctly!\")\n",
    "        else:\n",
    "            print(\"❌ ISSUE: Timestamps not mapped correctly\")\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(f\"❌ Import error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Function error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_timestamp_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b00f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f1d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4f276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17aa0dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravis\\OneDrive\\Desktop\\PROJECTS\\LLMProjectsFromLinkedin\\VideoQARAG\\videorag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Current chunker function source:\n",
      "============================================================\n",
      "✅ Chunker HAS citation tracking\n",
      "✅ Chunker HAS debug output\n",
      "\n",
      "Function length: 4900 characters\n",
      "First few lines:\n",
      "def process_transcription_for_rag_langchain(transcription_file: str, video_metadata: Dict) -> List[Dict]:\n",
      "    \"\"\"\n",
      "    Enhanced pipeline with citation tracking - handles both fast and slow transcript formats\n",
      "    \n",
      "    Args:\n",
      "        transcription_file: Path to transcription JSON file\n",
      "        video_metadata: Video information from download\n",
      "    \n",
      "    Returns:\n",
      "        List of processed chunks with citation tracking info\n"
     ]
    }
   ],
   "source": [
    "# Quick test to see what's in your chunker.py\n",
    "import inspect\n",
    "from chunker import process_transcription_for_rag_langchain\n",
    "\n",
    "print(\"🔍 Current chunker function source:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get the source code of the function\n",
    "try:\n",
    "    source = inspect.getsource(process_transcription_for_rag_langchain)\n",
    "    \n",
    "    # Check if it has citation tracking\n",
    "    if 'start_time' in source and 'find_chunk_timestamps' in source:\n",
    "        print(\"✅ Chunker HAS citation tracking\")\n",
    "    else:\n",
    "        print(\"❌ Chunker does NOT have citation tracking\")\n",
    "    \n",
    "    # Check if it has debug output\n",
    "    if 'Debug timestamp mapping' in source:\n",
    "        print(\"✅ Chunker HAS debug output\")\n",
    "    else:\n",
    "        print(\"❌ Chunker does NOT have debug output\")\n",
    "        \n",
    "    print(f\"\\nFunction length: {len(source)} characters\")\n",
    "    print(\"First few lines:\")\n",
    "    print(\"\\n\".join(source.split('\\n')[:10]))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videorag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
